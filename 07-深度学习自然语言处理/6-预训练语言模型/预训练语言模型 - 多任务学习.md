# 预训练语言模型 - 多任务学习





## 经验



多任务学习就更好玩了，目前主要有两大代表： MT-DNN 与 ERNIE 2.0。

- **MT-DNN** 又叫**联合训练**，其实就是将预训练语言模型用在多个任务中去接着预训练，从而提高模型泛化。具体来说，训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志， 然后shuffle 之后进行训练。

- **ERNIE**  提出一个很好的思路： **Continual Learning**。 这点很有意思，就像人类做题一样， 它不像 MT-DNN 那样训练，而是这样：

  ```
  task1 --> task1,task2 --> task1, task2, task3
  ```

  即在训练后续任务时，前面的任务依旧要参与训练，主要是希望在学习后续任务时依旧记得前面任务的学习成果。

我个人觉得 ERNIE 更符合我们人类的训练方式，不过具体的两种学习方式的表现还需要对比一下。 

回想我们人类的学习方式，其最初是专题训练，即每个 task 分别训练， 然后再进行总体训练，即所有 task 一起进行训练，然后发现自己的弱点，然后适当加强对某任务的训练，然后又进行总体训练，如此反复， 过程更像是这样：

```
(task1 or task 2 or task3)--> (task1, task2), (task1, task3), (task2, task3) --> (task1, task2, task3) --> (task1 or task2 or task3) --> ...
专题训练 --> 组合训练 --> 总体训练 --> 专题训练 --> ...
```

如果要保证训练新任务时不会过分忘记前面训练所得到的成果，似乎各个任务的训练样本比例以及训练时间更加重要。比如你做了一年的阅读理解，突然让你做单向选择，你答的也不会太好。

因此，我个人觉得， **联合训练 + Continual Learning** 是一个不错的思路。







## Reference

[如何利用深度学习模型实现多任务学习？这里有三点经验](https://zhuanlan.zhihu.com/p/56900939)

[Multi-task Learning(Review)多任务学习概述](https://zhuanlan.zhihu.com/p/59413549)



MT-BioNER: Multi-task Learning for Biomedical Named Entity Recognition using Deep Bidirectional Transformers

 MT-DNN: Multi-Task Deep Neural Networks for Natural Language Understanding