# 无监督文本匹配

### Skip Thought

Skip Thought是一种无监督学习的句子编码器模型，它通过预测上下文来训练一个句子的向量表示。具体来说，给定一个句子，Skip-Thought模型会尝试预测其前后相邻的句子。

假设我们有三个连续的句子：s_i-1, s_i, s_i+1。在Skip-Thought模型中，我们将s_i作为输入（encoder），并尝试预测s_i-1和s_i+1（decoder）。这样就可以得到每个单词或者整个句子的向量表示。



其实，相对于skip-gram的单纯预测上下文窗口内的词，skip-thought在没有任何语法信息、语义信息、上下文信息等重要特征的情况下，要预测上下文语句有点过于严苛了，而且 skip-thought 训练完后就抛弃解码器比较浪费。skip-thought的大量优化模型中，比较著名的是发表于2018ICLR的《AN EFFICIENT FRAMEWORK FOR LEARNING SENTENCE REPRESENTATIONS》中提出的框架，这个框架又叫**Quick-Thought**。不同于skip-thought中GRU解码器预测上下文句子，quick-thought是给定包含当前句真实上下文语句的几个训练语料库中的句子，然后来从中挑选出真正的上下文，即把skip-thought中的解码器语言模型换成了一个分类器，从而大大提升了编码器的训练效率以及训练效果，感兴趣的读者可以了解一下。



### FastSent

训练目标是预测句子上下文中的单词，通常是句子两边各自的一个单词。然而，这个设定并不是固定的，可以根据具体的应用场景和需求进行调整。



## Reference

[BERT模型可以使用无监督的方法做文本相似度任务吗？](https://www.zhihu.com/question/354129879)

