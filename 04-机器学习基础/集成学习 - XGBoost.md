# 集成学习 - XGBoost

---

**该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。**



Shrinkage 方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型。



Column Subsampling类似于随机森林中的选取部分特征进行建树。



集成学习 - XGBoost

XGBoost是一个开源的机器学习库，用于提升树模型的性能。它使用了一种称为“梯度提升”的技术来创建一个强大的模型，该模型由许多较弱的模型组合而成。XGBoost已被广泛应用于各种机器学习任务，包括分类、回归和排序。

XGBoost的一些主要优点包括：

* 它速度快，并且可以处理大型数据集。
* 它可以自动学习特征的重要性，并可以处理缺失值。
* 它支持正则化，这可以防止模型过拟合。

XGBoost的一些缺点包括：

* 它可能很难调参。
* 它可能对异常值敏感。

总的来说，XGBoost是一个强大的机器学习库，可以用于各种机器学习任务。它速度快，可以处理大型数据集，并且可以自动学习特征的重要性。然而，它可能很难调参，并且可能对异常值敏感。



XGBoost（Extreme Gradient Boosting）是一种基于梯度提升决策树的集成学习方法。其执行步骤如下：

1. 初始化：为训练数据设定一个初始预测值，通常设置为平均值。

2. 构建第一棵树：使用梯度提升算法构建第一棵决策树，以减小当前模型的预测误差。

3. 计算残差：计算每个样本点的真实值与当前模型预测值之间的差异，即残差。

4. 构建新的决策树：在现有模型上添加新的决策树，目标是进一步减少残差。

5. 更新模型：将新构造出来的这颗决策树加入到现有模型中，并更新所有样本点对应的预测值。

6. 重复步骤3-5直至满足停止条件。例如达到设定好了最大迭代次数或者误差已经小于某个阈值等等。

举例说明：
假设我们要用XGBoost进行房价预测。首先初始化一个初始预测（比如所有房子价格都是平均价格）。然后我们会构建第一棵决策树去尽可能地减少每个房子实际价格与预测价格之间的误差。接着，我们会计算每个房子的残差（实际价格-当前预测价格）。然后再构建一棵新的决策树去尽可能地减少这些残差。最后将新构造出来的这颗决策树加入到现有模型中，并更新所有样本点对应的预测值。重复以上步骤直至满足停止条件为止。
