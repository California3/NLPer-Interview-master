# AdamW

当结合权重衰减（Weight Decay）时，Adam优化器的更新规则会稍有不同，以确保权重衰减能够正确地应用到参数更新中。在这种情况下，权重衰减通常是直接应用于参数更新步骤中，而不是作为损失函数的一部分。这种方法被称为“Decoupled Weight Decay”。

在使用Adam优化器时，结合权重衰减的更新规则可以表示为：

1. 首先计算梯度的一阶矩（动量）和二阶矩（RMSprop）的估计：
   $$
   v_t = \beta_1 v_{t-1} + (1 - \beta_1) \nabla_w L \\
   s_t = \beta_2 s_{t-1} + (1 - \beta_2) (\nabla_w L)^2
   $$
   
2. 对一阶矩和二阶矩进行偏差校正：
   $$
   \hat{v}_t = \frac{v_t}{1 - \beta_1^t}\\
   \hat{s}_t = \frac{s_t}{1 - \beta_2^t}\
   $$
   
3. 应用权重衰减并更新参数：
   $$
   w_{t+1} = (1 - \eta \lambda) w_t - \frac{\eta}{\sqrt{\hat{s}_t} + \epsilon} \hat{v}_t
   $$
   

其中，\($\eta$\) 是学习率，\($\lambda$\) 是权重衰减系数，\($\epsilon$\) 是为了防止除以零的小常数。

通过这种方式，权重衰减被直接应用于参数更新步骤中，而不是通过损失函数来实现。这有助于保持权重衰减与自适应学习率之间的解耦，从而在使用自适应学习率优化器时更有效地控制模型的复杂度和防止过拟合。