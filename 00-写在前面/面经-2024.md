# 24 校招-面经

每一场基本都是一个小时多一点点，围绕实习、八股、算法展开，最后是反问。

### 小米

一面：

1. 自我介绍
2. 实习介绍，涉及技术原理
3. RAG、向量数据库
4. Transformer 架构 Encoder / Decoder 介绍
5. Mask-Self Attention 如何屏蔽未来 tokens？
6. 做题：统计数字出现频次，返回频次排名第 top k高的数字。
7. 反问

二面：

1. 自我介绍
2. LSTM 数学公式
3. Attention 为什么要除以根号 dk？
4. 为什么大模型n_ctx不做无限长？
5. RAG - 如何优化查询效率？
6. 向量数据库如何构建索引？
7. 做题：字符串最小编辑距离
8. 反问

### 蚂蚁

一面：

1. 相互介绍
2. 实习介绍
3. Transformer 架构 Encoder / Decoder 介绍
4. RAG 查询方法
5. PEFT 介绍 
6. RoPE
7. 为什么分类问题要用 Cross-Entropy 而不用 MSE？
8. 做题：整段序列：含两段上升序列，前一段元素比后一段都大。查询某元素的索引，但不允许直接遍历整个序列。
9. 反问

### 腾讯

一面：

1. 相互介绍
2. 实习介绍
3. Transformer 介绍
4. RAG 查询方法
5. PEFT 介绍 
6. 做题：统计逆序对
7. 做题：回溯+剪枝。取元素构建数，具体限制不记得了。
8. 反问

二面：

1. 相互介绍
2. 实习介绍
3. Llama 中文化，模型要做什么改动？怎么扩词表？
4. 多卡如何训练？
5. forward() 和 generate() 有什么区别？
6. 投机采样、拒绝采样、重采样
7. 提了一嘴 MOE
8. 做题：原创题，声韵母拼音纠错。
9. 没反问，因为做题前就下线了。让做完题把题解微信发给面试官。

三面：

1. 自我介绍
2. 实习，大概规模、输入输出、团队分工、有什么没解决的问题。
3. 大模型做类似 rank 或者打分的任务，是否可行？
4. 大模型幻觉，怎么产生的？因为什么？
5. 业务情景题：在与大模型对话过程，第 101 轮问完之后，会有第 102 轮还要将前面的上下文重新传回去的情况，如何优化/解决这个问题？
6. 你觉得自己还有什么缺点吗？
7. 反问

### 百度

一面：略

问的和 NLP 没啥关系，估计是数据工（面试官强烈承诺不是），问得很简单，都是如何处理数据，收集数据的问题，还有一点点批处理。没做题。面完微信告知通过，但选择主动向 HR 告知不推进二面了。

### 美团

和机器学习相关，没有和 NLP 非常紧密，大模型也没有。

一面：

做题 （2 道）- 八股 - 反问。略

二面：

八股 - 做题（2 道）- 反问。略