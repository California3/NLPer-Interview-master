**NLP** **大纲**

词向量，语言模型，预训练语言模型：NNLM， word2vec， （Glove， Fasttext） ，预训练语言模型深入：ELMO，GPT，GPT1，GPT2，ERINE，ERINE2.0，albert，Robert， xlnet 等变种。

Attention机制 - transorformer：Attention 机制深入： 各种花式Attention

Transformer， transformer vs lstm vs cnn

文本分类：Fattext， TextCNN，TextRNN， HAN， TextRCNN，TextGCN

预训练语言模型入门：BERT， BERT 用于文本分类

文本匹配：BERT用于文本匹配， 孪生网络（Siamese），ESIM， sentence-bert， https://zhuanlan.zhihu.com/p/87384188

序列标注：BERT用于序列标注， bi-lstm + crf， BERT+crf， Bert+bi-lstm+crf

中英文分词技术演变：BPE, wordpiece，ULM，sentencepiece

自然语言推理：

阅读理解：BiDAF，BERT，以及各种骚操作。

关键词提取： 从 tfidf 开始到SIFRank等

模型压缩与预训练模型压缩：知识蒸馏，DistillBert，tinybert等

预训练模型可解释性：

关系抽取：

机器翻译：从 seq2seq 到 ...

任务型对话：

开放域对话：

自然语言生成： 从seq2seq到BART

文本摘要：

文本纠错：

认知图谱构建：GIANT, 阿里巴巴认知图谱

多模态预训练：VideoBERT到最新的ERNIE-ViL

多模态融合：

 

 