# Recent Advances in Language Model Fine-tuning





## Reference 

[Universal Language Model Fine-tuning for Text Classification](https://www.aclweb.org/anthology/P18-1031.pdf)

[Pretrained Transformers Improve Out-of-Distribution Robustness](https://www.aclweb.org/anthology/2020.acl-main.244.pdf)

[Neural Transfer Learning for Natural Language Processing](https://ruder.io/thesis/neural_transfer_learning_for_nlp.pdf#page=62)

[An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://www.aclweb.org/anthology/N19-1213.pdf)

[Zero-Shot Entity Linking by Reading Entity Descriptions](https://www.aclweb.org/anthology/P19-1335.pdf)

[Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling](https://www.aclweb.org/anthology/D19-1433.pdf)

[Pretraining Methods for Dialog Context Representation Learning](https://www.aclweb.org/anthology/P19-1373.pdf)

[Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://www.aclweb.org/anthology/2020.acl-main.740.pdf)

